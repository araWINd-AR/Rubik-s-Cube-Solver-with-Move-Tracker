CSCI6660_AI
# Rubik-s-Cube-Solver-with-Move-Tracker
Implemented By Aravind Ganipisetty , Nandini Meka

This project involves two main files: move tracker agent.py and puzzle.py. The Agent file defines an Agent class that implements various methods to solve a cube puzzle using reinforcement learning. The agent keeps track of visited and revisited cube states and uses two key variables: cube, representing the current state of the cube (solved or scrambled), and QV, which holds the Q-values for different states. Initially, Q-values are randomly assigned and then updated based on experience. The starting state of the cube can be provided by the user either as a nested array (each array representing a cube face) or as a dictionary with each key-value pair representing one face and containing 12 values (4 rows × 3 columns per face, 6 faces in total). The agent also tracks the previous and second previous states since current decisions depend on them. These initial cube configurations are stored in a pattern database, and Q-values are linked using a hash function to uniquely identify states.

The cube-solving approach relies on a powerful reinforcement learning technique called Feature-based Q-learning. To assess progress toward solving the cube, a pattern database is used to evaluate how close a given state is to the goal. The Q-learning method is implemented with parameters such as the learning rate (alpha), discount factor (for future rewards), epsilon (for exploration), and a defined number of episodes. The algorithm uses an epsilon-greedy strategy, where it chooses between exploiting the best known action and exploring random actions. A random value is selected between 0 and 1; if it exceeds epsilon, the agent chooses the best action based on the current policy. If not, it picks a random action. In both cases, the Q-value for the current state-action pair is updated using the standard Q-learning formula: Q(s,a) = Q(s,a) + alpha * [reward + gamma * max(Q(s’,a’)) - Q(s,a)].

This cycle is repeated over multiple episodes, where the agent makes moves, evaluates the results, receives rewards, and updates its policy. The reward for reaching the goal state is set to 100, while non-goal moves incur a small penalty (typically 0.1). Additional penalties are applied if the cube regresses (e.g., if fewer sides are solved in the new state than in the previous one). For each step, the agent calculates the total reward for all possible next actions and selects the one with the maximum expected value. The process continues until the cube is solved or the maximum number of episodes is reached, at which point the agent converges and exits.

The `Puzzle.py` file contains helper functions for the cube logic. It includes methods for shuffling the cube (used when no initial state is provided), checking whether the cube is in a solved state, and handling valid moves and state transitions. The variable n determines how many random moves should be applied to initially scramble the cube. The implementation supports 180-degree rotations, which helps control the branching factor in the cube’s state space, making the learning process more efficient.

A reinforcement learning (RL) agent can solve the Rubik's Cube using Q-learning, learning optimal moves without relying on pre-defined algorithms. The agent refines its decisions using Q-values, balancing exploration of new moves with exploitation of known good moves. Transparent move documentation allows detailed analysis of the learning process. 
